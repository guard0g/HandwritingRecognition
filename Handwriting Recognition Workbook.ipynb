{
 "metadata": {
  "name": "",
  "signature": "sha256:120070d7ba5aa11f9d96767d6773a7dceaa8a96f025969b9e71b21ba7fc56dee"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Neural Networks in the Wild - Handwriting Recognition\n",
      "  \n",
      "By John Liu  \n",
      "  \n",
      "##Introduction\n",
      "  \n",
      "This ipython notebook investigates the topic of handwriting recognition. It begins with a short OpenCV tutorial on character segmentation. Using the NIST SD-19 dataset, various linear and ensemble estimators from scikit-learn are investigated for character recognition. These are compared to several neural network models implemented in pylearn2 (without YAML).  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Character Segmentation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Character segmentation consists of several preprocessing steps after which contour detection and bounding rectangles are extracted. Typical preprocessing consists of:  \n",
      "\n",
      "* threshold pixels\n",
      "* apply smoothing filter\n",
      "* morphological filtering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "import cv2\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Load image"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "img = \"Plate.jpg\"   # License plate\n",
      "#img = \"Food.jpg\"   # motto\n",
      "\n",
      "dir = os.curdir\n",
      "\n",
      "path = os.path.join(dir,img)\n",
      "raw_image = cv2.imread(path,0)\n",
      "\n",
      "cv2.imshow(\"original\",raw_image)\n",
      "plt.subplot(2,3,1)\n",
      "plt.title(\"Original\")\n",
      "plt.imshow(raw_image,'gray')\n",
      "plt.xticks([]),plt.yticks([]);\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Apply Smoothing with Kernel"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sm_image = cv2.blur(raw_image,(8,8))\n",
      "\n",
      "cv2.imshow(\"smoothed\",sm_image)\n",
      "\n",
      "plt.subplot(2,3,2)\n",
      "plt.title(\"Smoothed\")\n",
      "plt.imshow(sm_image,'gray')\n",
      "plt.xticks([]),plt.yticks([]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Thresholding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cv2.imshow(\"smoothed\",sm_image)\n",
      "ret,bw_image = cv2.threshold(sm_image,160,255,cv2.THRESH_BINARY_INV)\n",
      "\n",
      "cv2.imshow(\"thresholded\",bw_image)\n",
      "plt.subplot(2,3,3)\n",
      "plt.title(\"Thresholded\")\n",
      "plt.imshow(bw_image,'gray')\n",
      "plt.xticks([]),plt.yticks([]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Open (Erode and Dilate) image to remove Noise\n",
      "\n",
      " \n",
      "###Erode"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernel = np.ones((4,4),np.uint8)\n",
      "er_image = cv2.erode(bw_image,kernel)\n",
      "\n",
      "cv2.imshow(\"eroded\",er_image)\n",
      "plt.subplot(2,3,4)\n",
      "plt.title(\"Eroded\")\n",
      "plt.imshow(er_image,'gray')\n",
      "plt.xticks([]),plt.yticks([]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Then dilate"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "kernel = np.ones((2,2),np.uint8)\n",
      "di_image = cv2.dilate(er_image,kernel)\n",
      "\n",
      "cv2.imshow(\"dilated\",di_image)\n",
      "plt.title(\"Dilated\")\n",
      "plt.subplot(2,3,5)\n",
      "plt.imshow(di_image,'gray')\n",
      "plt.xticks([]),plt.yticks([]);\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Find Contours"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mo_image = di_image.copy()\n",
      "contour0 = cv2.findContours(mo_image.copy(),cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
      "contours = [cv2.approxPolyDP(cnt,3,True) for cnt in contour0[0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Extract Bounding Rectangles"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "maxArea = 0\n",
      "rect=[]\n",
      "for ctr in contours:\n",
      "    maxArea = max(maxArea,cv2.contourArea(ctr))\n",
      "\n",
      "if img == \"Food.jpg\":\n",
      "    areaRatio = 0.05\n",
      "elif img == \"Plate.jpg\":\n",
      "    areaRatio = 0.5\n",
      "    \n",
      "for ctr in contours:    \n",
      "    if cv2.contourArea(ctr) > maxArea * areaRatio: \n",
      "        rect.append(cv2.boundingRect(cv2.approxPolyDP(ctr,1,True)))\n",
      "    \n",
      "symbols=[]\n",
      "for i in rect:\n",
      "    x = i[0]\n",
      "    y = i[1]\n",
      "    w = i[2]\n",
      "    h = i[3]\n",
      "    p1 = (x,y)\n",
      "    p2 = (x+w,y+h)\n",
      "    cv2.rectangle(mo_image,p1,p2,255,2)\n",
      "    image = cv2.resize(mo_image[y:y+h,x:x+w],(32,32))\n",
      "    symbols.append(image.reshape(1024,).astype(\"uint8\"))\n",
      "\n",
      "testset_data = np.array(symbols)\n",
      "\n",
      "cv2.imshow(\"segmented\",mo_image)\n",
      "plt.subplot(2,3,6)\n",
      "plt.title(\"Segmented\")\n",
      "plt.imshow(mo_image,'gray')\n",
      "plt.xticks([]),plt.yticks([]);\n",
      "#plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# garbage collection\n",
      "cv2.destroyAllWindows()\n",
      "plt.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show glyphs\n",
      "for i in range(len(symbols)):\n",
      "    image = np.zeros(shape=(64,64))\n",
      "    image[15:47,15:47] = symbols[i].reshape((32,32))\n",
      "    cv2.imshow(\"sym\",image)\n",
      "    cv2.waitKey(0)\n",
      "\n",
      "cv2.destroyAllWindows()\n",
      "plt.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SD-19 Dataset preprocessing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Cut slice of SD19 dataset"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import cv2\n",
      "import os\n",
      "\n",
      "\n",
      "dir = os.curdir\n",
      "path = os.path.join(dir,\"sd19/sd19-binary.txt\")\n",
      "f = open(path,\"r\")\n",
      "\n",
      "target=[]\n",
      "data=[]\n",
      "for i in range(100000):\n",
      "    line = f.readline()\n",
      "    target.append(line[0])\n",
      "    image = np.array([((int(line[2+j])+1)%2) for j in range(16384)]).astype(\"uint8\")\n",
      "    r_image = cv2.resize(image.reshape((128,128)),(32,32)).reshape(1024,)\n",
      "    data.append(r_image)\n",
      "dataset = pd.DataFrame(data)\n",
      "dataset['target']=target\n",
      "dataset.to_csv(os.path.join(dir,\"sd19.csv\"),index=False,header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Make a modified set for pylearn2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# preprocess dataset to reorder columns to place label in first column for pylearn2\n",
      "import os\n",
      "import pandas as pd\n",
      "dir = os.curdir\n",
      "path = os.path.join(dir,\"sd19.csv\") \n",
      "df = pd.read_csv(path,header=None)\n",
      "\n",
      "cols = df.columns.tolist()\n",
      "cols = cols[-1:] + cols[:-1]\n",
      "df = df[cols]\n",
      "symbol_map = dict([(chr(x), x) for x in range(48,58) + range (65,91) + range(97,123)])\n",
      "df[1024]=[symbol_map[i] for i in df[1024]]\n",
      "df.to_csv(\"sd19n.csv\",index=False,header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###Load Slice of SD19 (100k samples of 62 classes in [[0-9][A-Z][a-z]]"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import pandas as pd\n",
      "dir = os.curdir\n",
      "path = os.path.join(dir,\"sd19.csv\") \n",
      "df = pd.read_csv(path,header=None)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Classification"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
      "from sklearn import linear_model,svm\n",
      "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,AdaBoostClassifier\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "symbol_map = dict([(chr(x), x) for x in range(48,58) + range (65,91) + range(97,123)])\n",
      "(trainX, testX, trainY, testY) = train_test_split(df.ix[:,0:1023] ,df.ix[:,1024].replace(symbol_map) , test_size = 0.3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Logistic Regression (SoftMax, MaxEnt, Logit)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modelSG = linear_model.SGDClassifier(loss='log',penalty='l2',alpha=0.001,n_iter=1,shuffle=False,n_jobs=-1,random_state=33)\n",
      "modelSG.fit(trainX,trainY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = modelSG.predict(testX)\n",
      "print accuracy_score(testY,pred)\n",
      "#print classification_report(testY,pred)\n",
      "#print confusion_matrix(testY,pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## SVM with Radial Basis Function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#modelSVC = svm.SVC(kernel='linear',random_state=33)\n",
      "modelSVC = svm.SVC(kernel='rbf',gamma=0,tol=0.01,verbose=False,max_iter=-1,random_state=33)\n",
      "modelSVC.fit(trainX,trainY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = modelSVC.predict(testX)\n",
      "print accuracy_score(testY,pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Ensemble: Random Forest"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modelRF = RandomForestClassifier(n_estimators=10,criterion='gini',max_depth=None,n_jobs=-1,verbose=0,random_state=33)\n",
      "modelRF.fit(trainX,trainY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = modelRF.predict(testX)\n",
      "print accuracy_score(testY,pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Emsemble: Extra Trees"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modelET = ExtraTreesClassifier(n_estimators=10,max_depth=None,random_state=33)\n",
      "modelET.fit(trainX,trainY)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = modelET.predict(testX)\n",
      "print accuracy_score(testY,pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print classification_report(testY,pred)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Neural Networks using pylearn2"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import numpy as np\n",
      "\n",
      "from pylearn2.train import Train\n",
      "from pylearn2.models import softmax_regression, mlp\n",
      "from pylearn2.training_algorithms import bgd, sgd\n",
      "from pylearn2.termination_criteria import MonitorBased\n",
      "from pylearn2.termination_criteria import EpochCounter\n",
      "from pylearn2.train_extensions import best_params\n",
      "from pylearn2.utils import serial\n",
      "from pylearn2.costs.mlp.dropout import Dropout\n",
      "from pylearn2.datasets.dense_design_matrix import DenseDesignMatrix\n",
      "from pylearn2.datasets import csv_dataset\n",
      "from theano import function\n",
      "from theano import tensor as T\n",
      "import theano\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load datasets\n",
      "train = csv_dataset.CSVDataset(\"sd19n.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',',end_fraction=0.7)\n",
      "valid = csv_dataset.CSVDataset(\"valid.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',')\n",
      "test = csv_dataset.CSVDataset(\"test.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2-layer SoftMax with sigmoid hidden_layer = 400"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Simple ANN\n",
      "\n",
      "h0 = mlp.Sigmoid(layer_name=\"h0\",dim=400, sparse_init=20)\n",
      "y0 = mlp.Softmax(n_classes=62,layer_name=\"y0\",sparse_init=20)\n",
      "layers = [h0, y0]\n",
      "\n",
      "nn = mlp.MLP(layers,nvis=1024)\n",
      "algo = sgd.SGD(learning_rate=0.05,batch_size=100,monitoring_dataset=valid,termination_criterion=EpochCounter(100))\n",
      "algo.setup(nn,train)\n",
      "\n",
      "save_best = best_params.MonitorBasedSaveBest(channel_name=\"y0_misclass\",save_path='best_params.pkl')\n",
      "while True:\n",
      "    algo.train(dataset=train)\n",
      "    nn.monitor.report_epoch()\n",
      "    nn.monitor()\n",
      "    save_best.on_monitor(nn,train,algo)\n",
      "    if not algo.continue_learning(nn):\n",
      "        break\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate accuracy on test set\n",
      "\n",
      "nn = serial.load(\"best_params.pkl\")\n",
      "\n",
      "inputs = test.X.astype(\"float32\")\n",
      "yhat = nn.fprop(theano.shared(inputs,name='inputs')).eval()\n",
      "\n",
      "count = 0.\n",
      "for i in range(test.X.shape[0]):\n",
      "    if np.argmax(test.y[i])==np.argmax(yhat[i]):\n",
      "        count += 1.\n",
      "\n",
      "print \"accuracy = \", count / test.X.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3-layer SoftMax with Softplus hidden_layer = 800 with Dropout"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load dataset\n",
      "\n",
      "train = csv_dataset.CSVDataset(\"sd19n.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',',end_fraction=0.7)\n",
      "valid = csv_dataset.CSVDataset(\"valid.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',')\n",
      "test = csv_dataset.CSVDataset(\"test.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SoftPlus with Dropout\n",
      "\n",
      "h0 = mlp.Softplus(layer_name='h0', dim=800, sparse_init=40)\n",
      "h1 = mlp.Softplus(layer_name='h1', dim=800, sparse_init=40)\n",
      "y0 = mlp.Softmax(layer_name='y0', n_classes=62, irange=0)\n",
      "layers = [h0, h1, y0]\n",
      "\n",
      "model = mlp.MLP(layers, nvis=1024)\n",
      "\n",
      "monitoring = dict(valid=valid)\n",
      "termination = MonitorBased(channel_name=\"valid_y0_misclass\", N=10)\n",
      "extensions = [best_params.MonitorBasedSaveBest(channel_name=\"valid_y0_misclass\",\n",
      "save_path=\"train_best.pkl\")]\n",
      "\n",
      "algorithm = sgd.SGD(0.1, batch_size=100, cost=Dropout(),\n",
      "                    monitoring_dataset = monitoring, termination_criterion = termination)\n",
      "\n",
      "print 'Running training'\n",
      "train_job = Train(train, model, algorithm, extensions=extensions, save_path=\"train.pkl\", save_freq=1)\n",
      "train_job.main_loop()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate accuracy on test set\n",
      "\n",
      "model = serial.load(\"train_best.pkl\")\n",
      "\n",
      "inputs = test.X.astype(\"float32\")\n",
      "yhat = model.fprop(theano.shared(inputs,name='inputs')).eval()\n",
      "\n",
      "count = 0.\n",
      "for i in range(test.X.shape[0]):\n",
      "    if np.argmax(test.y[i])==np.argmax(yhat[i]):\n",
      "        count += 1.\n",
      "\n",
      "print \"accuracy = \", count / test.X.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2-layer Softmax with RectLinear hidden_layer = 400 with Momentum"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Rectified Linear with Momentum\n",
      "\n",
      "from pylearn2.training_algorithms import sgd, learning_rule\n",
      "\n",
      "h0 = mlp.RectifiedLinear(layer_name='h0', dim=400, sparse_init=40)\n",
      "y0 = mlp.Softmax(layer_name='y0', n_classes=62, irange=0)\n",
      "layers = [h0, y0]\n",
      "\n",
      "model = mlp.MLP(layers, nvis=1024)\n",
      "\n",
      "# momentum\n",
      "initial_momentum = 0.5\n",
      "final_momentum = 0.99\n",
      "start = 1\n",
      "saturate = 50\n",
      "momentum_rule = learning_rule.Momentum(initial_momentum)\n",
      "\n",
      "monitoring = dict(valid=valid)\n",
      "termination = MonitorBased(channel_name=\"valid_y0_misclass\", N=10)\n",
      "extensions = [best_params.MonitorBasedSaveBest(channel_name=\"valid_y0_misclass\",save_path=\"rect_best.pkl\"),\n",
      "              learning_rule.MomentumAdjustor(final_momentum,start,saturate)]\n",
      "\n",
      "algorithm = sgd.SGD(0.1, batch_size=100, cost=Dropout(),learning_rule=momentum_rule,\n",
      "                    monitoring_dataset = monitoring, termination_criterion = termination)\n",
      "\n",
      "print 'Running training'\n",
      "train_job = Train(train, model, algorithm, extensions=extensions, save_path=\"rect.pkl\", save_freq=5)\n",
      "train_job.main_loop()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate accuracy on test set\n",
      "\n",
      "model = serial.load(\"rect_best.pkl\")\n",
      "\n",
      "inputs = test.X.astype(\"float32\")\n",
      "yhat = model.fprop(theano.shared(inputs,name='inputs')).eval()\n",
      "\n",
      "count = 0.\n",
      "for i in range(test.X.shape[0]):\n",
      "    if np.argmax(test.y[i])==np.argmax(yhat[i]):\n",
      "        count += 1.\n",
      "\n",
      "print \"accuracy = \", count / test.X.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Stacked Denoising Autoencoders"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train = csv_dataset.CSVDataset(\"sd19n.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',',end_fraction=0.7)\n",
      "valid = csv_dataset.CSVDataset(\"valid.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',')\n",
      "test = csv_dataset.CSVDataset(\"test.csv\",one_hot=True,expect_labels=True,expect_headers=False,delimiter=',')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#layer 1\n",
      "\n",
      "corruptor = BinomialCorruptor(corruption_level=0.2)\n",
      "model = autoencoder.DenoisingAutoencoder(nvis=1024,nhid=500,irange=0.05,corruptor=corruptor,act_enc=\"tanh\",act_dec=None)\n",
      "algorithm = sgd.SGD(learning_rate=0.001, batch_size=100, monitoring_batches=5, cost=MeanSquaredReconstructionError(),\n",
      "                    monitoring_dataset=train, termination_criterion=EpochCounter(10))\n",
      "\n",
      "print 'Running training'\n",
      "train_job = Train(train, model, algorithm, save_path=\"dae_layer1.pkl\", save_freq=1)\n",
      "train_job.main_loop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# layer 2\n",
      "\n",
      "transformer = serial.load(\"dae_layer1.pkl\")\n",
      "ntrain = TransformerDataset(raw=train,transformer=transformer)\n",
      "\n",
      "corruptor = BinomialCorruptor(corruption_level=0.3)\n",
      "model = autoencoder.DenoisingAutoencoder(nvis=500,nhid=500,irange=0.05,corruptor=corruptor,act_enc=\"tanh\",act_dec=None)\n",
      "algorithm = sgd.SGD(learning_rate=0.001, batch_size=100, monitoring_batches=5, cost=MeanSquaredReconstructionError(),\n",
      "                    monitoring_dataset=ntrain, termination_criterion=EpochCounter(10))\n",
      "\n",
      "print 'Running training'\n",
      "train_job = Train(ntrain, model, algorithm, save_path=\"dae_layer2.pkl\", save_freq=1)\n",
      "train_job.main_loop()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# stacking and supervised fine-tuning\n",
      "\n",
      "dae1 = serial.load(\"dae_layer1.pkl\")\n",
      "dae2 = serial.load(\"dae_layer2.pkl\")\n",
      "\n",
      "h1 = mlp.PretrainedLayer(layer_name='h1',layer_content=dae1)\n",
      "h2 = mlp.PretrainedLayer(layer_name='h2',layer_content=dae2)\n",
      "y0 = mlp.Softmax(layer_name='y0', n_classes=62, irange=0.005,max_col_norm=1.9365)\n",
      "layers = [h1, h2, y0]\n",
      "\n",
      "monitoring = dict(valid=valid)\n",
      "callback = sgd.ExponentialDecay(decay_factor=1.00004,min_lr=0.000001)\n",
      "extensions = [best_params.MonitorBasedSaveBest(channel_name=\"valid_y0_misclass\",save_path=\"dae_best.pkl\"),\n",
      "              learning_rule.MomentumAdjustor(final_momentum=0.7,start=1,saturate=250)]\n",
      "model = mlp.MLP(layers, batch_size=100, nvis=1024)\n",
      "momentum = learning_rule.Momentum(init_momentum=0.5)\n",
      "algorithm = sgd.SGD(learning_rate=0.05, batch_size=100,\n",
      "                    update_callbacks=callback, learning_rule=momentum,\n",
      "                    monitoring_dataset=monitoring, termination_criterion=EpochCounter(400))\n",
      "\n",
      "print 'Running training'\n",
      "train_job = Train(train, model, algorithm, extensions=extensions, save_path=\"dae.pkl\", save_freq=1)\n",
      "train_job.main_loop()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calculate accuracy on test set\n",
      "\n",
      "model = serial.load(\"dae_best.pkl\")\n",
      "\n",
      "inputs = test.X.astype(\"float32\")\n",
      "yhat = model.fprop(theano.shared(inputs,name='inputs')).eval()\n",
      "\n",
      "count = 0.\n",
      "for i in range(test.X.shape[0]):\n",
      "    if np.argmax(test.y[i])==np.argmax(yhat[i]):\n",
      "        count += 1.\n",
      "\n",
      "print \"accuracy = \", count / test.X.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}